%************************************************
\chapter{Hidden Markov Random Field based clustering for single cell gene expression data}\label{ch:clustering} 
%************************************************
\section{Markov Random Field prior distribution}
Let $S$ be a finite set of sites, each of which represents one ``cube" of data (see the results section for a detailed description of the data). Given the coordinates of each site, we were able to define a neighbourhood system on $S$ using a first order neighbourhood system, i.e the 6 closest sites. $S$ and its neighbourhood system can be viewed as a connecting graph $G$. Let $C$ be the set of cliques of $G$. $C$ is therefore the set of all sites that are all neighbours from one another.\\

Let a Random Field $Z$ be defined as a set of random variables $Z = \{Z_i , \forall i \in S\}$ each $Zi$ taking its value in $[1,K]$. For every site $i \in S$, let $N(i)$ represent the set of its neighbours and $\mathbf{z_{S-\{i\}}}$ a realization of the field restricted to $S-\{i\} = \{j \in S, j \neq i\}$. $Z$ is a Markov Random Field if and only if it verifies the Markov property at every site :
\begin{align}
\label{eq:markov1}
\forall i \in S, P_G (z_i \mid z_{S-\{i\}}) = P_G (z_i \mid z_j , j \in N(i))
\end{align}
Equation (\ref{eq:markov1}) states that the realization of the field at any site $i \in S, z_i$ can be fully determined using only the state of its neighbours $N(i)$. In other words each ``cube" is only dependent upon its neighbours.\\
The Hammersley-Clifford theorem state that if $Z$ is a Markov Random Field, the join distribution of the field follows a Gibbs distribution so that :
\begin{align}
\label{eq:prior}
P_G (\mathbf{z}) = \frac{e^{-H(\mathbf{z})}}{\sum\limits_{\mathbf{z'}} e^{-H(\mathbf{z'})}}
\end{align}
$H(\mathbf{z})$ is called the Energy function and is summed over the cliques of the graph $C$. Considering that we are working with an order one neighbouring graph, $C$ is the set of all the couples of sites $(i,j)$ that are neighbours. We chose to consider $H$ as a function of vector $\mathbf{\beta} = (\beta_1, \hdots, \beta_K)$ containing $K$ parameters, one per cluster and $v_{i,j}$ a potential function set to 1 in our method. 
\begin{align}
\label{eq:Energy}
H(\mathbf{z}) = - \sum\limits_{i \in S}\beta_{z_i}\sum_{\substack{i,j\\neighbours}}  v_{i,j} \times _{[z_i = z_j]}
\end{align}
The denominator in (\ref{eq:prior}) where $\mathbf{z'}$ represents all the possible realizations of the field is a normalizing constant that we will refer to as $W(\mathbf{\beta})$.\\
This model is closely related to a K-color Potts model \cite{Wu82} although instead of a single parameter $\beta$ for the entire model, we assign one $\beta$ per cluster. Equation (\ref{eq:Energy}) is a decreasing function of every component of $\mathbf{\beta}$ and of the number of neighbouring ``cubes" in the field having the same class. This Energy thus favours spatially regular partitions and a higher value of $\beta_h$, with $1 \leq h \leq K $ will amplify the smoothing effect, or coherence over cluster $h$. We chose to use one spatial smoothness parameter per cluster because of the nature of the data we are dealing with. Indeed, in a biological context, it is expected that some tissues will be more spatially coherent than others.\\

From this prior distribution we have $K$ unknown parameters $\mathbf{\beta} = (\beta_1, \hdots, \beta_K)$ to be estimated by the model. It is important to note at this point that $W(\mathbf{\beta})$ is summed over all possible realizations of the field $Z$, this is an exponentially complex sum as the cardinality of $S$ rises. Therefore the computation of the normalizing factor becomes intractable very quickly. To address this problem, we are going to need to make some approximations in order to compute this quantity (see Mean Field Approximations).\\

We have described the prior distribution of a Markov Random Field representing our partition, we now need to describe the relationship between $Z$ and the data.

\section{Hidden Markov model}
As $Z$ is unknown a priori and represents the partition, let $Y$ be a set of random variables representing the observations (the in-situ hybridization data). We have to assume conditional independence of the observations given the partition $Z$ so that, with $f_{z_i}$ the density function relative to cluster $z_i, i \in S$:
\begin{align}
p(\mathbf{y} \mid \mathbf{z} ; \Theta) &= \prod_{i \in S} p(y_i \mid z_i ; \Theta)\\
\label{eq:independence}
&= \prod_{i \in S} f_{z_i} (y_i \mid z_i ; \Theta)
\end{align}
We define one unknown parameter per cluster: $\Theta = (\mathbf{\theta_1},...,\mathbf{\theta_K})$. It is interesting to note that this part of the model is equivalent to an independent mixture model \cite{Mclachlan04}. Indeed, hidden Markov models can be viewed as independent mixture models where $Z$ is a set of independent, identically distributed random variables, which happens when $\beta = 0$.\\

Because the 169 genes chosen by Tomer et al. \cite{Tomer10} are key genes involved in the early development of Platynereis' brain, the assumption of conditional independence given the realization of the field seems acceptable. The validity of this hypothesis may however be argued for future RNA-seq datasets representing entire transcriptomes (see discussion).\\
Given a particular cluster $h \in [1,K]$ and M the set of considered genes, we assume that each gene $m \in M$ follows a Bernoulli distribution with parameter $\theta_{h,m}$. We then have one unknown Bernoulli parameter per gene per cluster so that :
\begin{align*}
\Theta &= (\mathbf{\theta_1},...,\mathbf{\theta_K})\\
&= \left( \begin{array} {ccc}
\theta_{1,1} & \ldots  & \theta_{1,_K}\\
\vdots & \ddots & \vdots\\
\theta_{M,1} & \ldots & \theta_{M,_K} \end{array} \right)
\end{align*}
The conditional density function $f_i, i \in S$ can be expressed as :
\begin{align}
f_i(y_i \mid z_i ; \Theta) = f_i(y_i \mid z_i ; \mathbf{\theta_{z_i}}) = \prod_{m \in M} \theta_{z_i,m}^{y_{i,m}} \times (1-\theta_{z_i,m}^{1-y_{i,m}})
\end{align}
Looking at both fields $Z$ and $Y \mid Z$ together, the complete likelihood of the model is expressed as :
\begin{align}
\label{eq:likelihood}
P_G(\mathbf{y},\mathbf{z} \mid \Theta, \mathbf{\beta}) &= f(\mathbf{y} \mid \mathbf{z}, \Theta)P_G(\mathbf{z} \mid \mathbf{\beta}) = \frac{exp\{{-H(\mathbf{z} \mid \mathbf{\beta})} + \sum\limits_{i \in S}log f_i(y_i \mid z_i, \theta_zi)\}}{\sum\limits_{z'} e^{-H(\mathbf{z})}}
\end{align}
Because equation (\ref{eq:likelihood}) is a Gibbs distribution, using the Hammersley-Clifford theorem we can conclude that the conditional field $Y$ given $Z =\mathbf{z}$ is another a Markov Random Field with the Energy function 
\[H(\mathbf{z} \mid \mathbf{y}, \mathbf{\beta}, \Theta) = H(\mathbf{z} \mid \mathbf{\beta}) - \sum\limits_{i \in S} log f_i(y_i \mid z_i, \Theta)\]

In our case, the goal is to recover the unknown realization of $Z: \mathbf{z}$. To this end we need to maximize the values of all the parameters of the model $\Phi = (\Theta, \mathbf{\beta})$. We will also need to determine the unknown value $K$. This will be determined a posteriori by computing the BIC over the model full likelihood \cite{Schwarz78}.

\section{Parameter estimation using the EM algorithm}
The EM principle can be applied to estimate the parameters $\Phi = (\Theta, \beta)$ of the hidden MRF model. After initializing the clusters $\mathbf{z}$,  we choose $\Phi^{l+1}$ at iteration $(l+1)$ in order to maximize the model's expectation:
\begin{align}
Q(\Phi \mid \Phi^{l}) &= \sum\limits_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{y} ; \Phi^{l}) log\:p(\mathbf{y},\mathbf{z};\Phi) \nonumber\\ 
\label{eq:decomposed}
&= \underbrace{\sum\limits_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{y} ; \Phi^{l}) log\:p(\mathbf{y} \mid \mathbf{z} ; \Theta)}_{R_y(\Theta\mid \Phi^l)} + \underbrace{\sum\limits_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{y} ; \Phi^{l}) log\:p(\mathbf{z} \mid \mathbf{\beta})}_{R_z(\mathbf{\beta}\mid \Phi^l)}
\end{align}
The decomposition in (\ref{eq:decomposed}) allows us to consider separately the maximization of $R_y(\Theta\mid \Phi^l)$ and $R_z(\mathbf{\beta}\mid \Phi^l)$:
\begin{align*}
\Theta^{l+1} &= arg\:\underset{\Theta}{max}\:R_y(\Theta\mid \Phi^l)\\
\mathbf{\beta}^{l+1} &= arg\:\underset{\mathbf{\beta}}{max}\:R_z(\mathbf{\beta}\mid \Phi^l)
\end{align*}
We estimate $Q(\Theta \mid \Phi^{l})$ in the E step by further developing it using equation (\ref{eq:independence}):
\begin{align*}
Q(\Theta \mid \Phi^{l}) &= R_y(\Theta\mid \Phi^l) = \sum\limits_{\mathbf{z}} p(\mathbf{z}\mid \mathbf{y} ; \Phi^{l})\;\sum\limits_{i \in S}\: log\: f_{z_i} (y_i ; \Theta)\\
&= \sum\limits_{i \in S} \sum\limits_{h=1}^{K}\; \left[log\: f_{h} (y_i  ; \Theta) \right] \; p(Z_i = h \mid \mathbf{y};\Phi^{l})
\end{align*}  
Therefore, at each iteration we need to compute in the E step the following quantity :
\begin{align*}
t_{i\,h}^{m+1} = p(Z_i = h \mid \mathbf{y};\Phi^{l})
\end{align*}
Computing this conditional probability is problematic because of the dependence between neighbouring ``cubes", and an exact value cannot be obtained without considerable computing resources. As mentioned previously, we also need to approximate the normalizing constant $W(\mathbf{\beta})$. Approximation methods include Besag's pseudo-likelihood \cite{Besag75} to compute $W(\mathbf{\beta})$, and simulating the posterior distribution of $Z$ given $\mathbf{y}$ with the parameters at iteration $(l)$, with a Gibbs sampler to estimate $t_{i\,h}^{m+1}$ \cite{Chalmond89}.\\

However, another method exists, the mean field approximation originally proposed in the field of statistical mechanics. Since then, it has been used in a variety of fields including computer vision \cite{Yuille90} and more recently to approximate the distribution of both $W(\mathbf{\beta})$ (with a single $\beta$) and $t_{i\,h}^{m+1}$ \cite{Zhang92}. We present here the extension of this method to a model with $\mathbf{\beta} = (\beta_1, \hdots, \beta_K)$.

\section{Mean field approximations}

The idea behind this approximation is to compute intractable quantities at any point $i \in S$ by setting the values of all the other sites in the field to their mean values. As seen in equation (\ref{eq:markov1}) in the case of a MRF, this is equivalent to fixing the values of $N(i)$ only.\\

When computing $t_{i\,h}^{m+1}$, the mean fields approximation yields the following fixed point equation \cite{Dang98} for $i \in S$ and $1 \leq h \leq K$ :
\begin{align}
\label{eq:fixedpoint}
t_{i\,h}^{m+1} \approx \frac{f_{h} (y_i;\mathbf{\theta}_{h}^m)\; exp\{\beta_h^m \: \sum_{j \in N(i)} t_{j\,h}^{m+1}\}}{\sum_{u=1}^K \: f_{u} (y_i;\mathbf{\theta}_{u}^m)\; exp\{\beta_u^m \: \sum_{j \in N(i)} t_{j\,u}^{m+1}\}}
\end{align}

For the normalizing constant $W(\mathbf{\beta})$, if we apply the mean-field approximation, using equation (\ref{eq:Energy}), we can write :
\begin{align*}
W(\mathbf{\beta}) = \sum\limits_{\mathbf{z'}} exp(-H(\mathbf{z'})) \approx \sum\limits_{i \in S}\;\sum\limits_{\mathbf{z_i}} exp(-H(\mathbf{z_i})) = \sum\limits_{i \in S}\;\sum\limits_{\mathbf{z_i}} exp(\beta_{z_i}\sum\limits_{N(i)}[z_i=z_j])
\end{align*}
With this new set of equations, we are now able to estimate all quantities needed in the E step to maximize the model's expectation.\\

\section{Maximization}
After the E step, the maximizing $\Phi$ is relatively straight forward. For $\Theta$, once the the $t_{i\,h}^{m} = p(Z_i = h \mid \mathbf{y};\Phi^{l})$ have been computed during the E-step, we use those probabilities to assign each cell to its cluster at step $l$. Once the new partition is created, the maximization of $\Theta$ can be computed iteratively for cluster $h \in [1,K]$ and gene $m \in M$ with $Expr_{h,m}$ the number of cells expressing gene $m$ in cluster $h$ and $Num_h$ the total number of cells in cluster $h$.
\begin{align*}
\theta_{m,h}^{l+1} = arg\:\underset{\Theta}{max}\:R_y(\Theta\mid \Phi^l) = \frac{Expr_{h,m}}{Num_h}
\end{align*}
We then need to maximize $\mathbf{\beta}^{l+1}$, to this end, we use a gradient ascent algorithm  for each $\beta_h^{l+1}, h \in [1,K]$ on the function $R_z(\mathbf{\beta}\mid \Phi^l)$. This process is done iteratively. (CITE LAMIAE)\\

We are now able to compute a partition over $K$ clusters by applying the previously described EM algorithm. However, we still need to find a way of choosing $K$.

\section{Estimating K}
Without any prior knowledge, choosing the right number of clusters $K$ is challenging. We decided to use an a posteriori method relying on the final log Likelihood of the model derived from equation (\ref{eq:likelihood}):
\begin{align*}
log\;L(\Phi) = 	log\;P_G(\mathbf{y},\mathbf{z} \mid \Theta, \mathbf{\beta})
\end{align*}
Because $Log\;L(\Phi)$ monotonically increases with the number of parameters of the model, the BIC approach penalizes the addition of new parameters to the model. Let $P$ be the total number of parameters in the model and $N$ the cardinality of $S$, the BIC is expressed as:
\begin{align*}
\label{eq:BIC}
- 2\; log\:L(\Phi) + P\:log\:N
\end{align*}
By computing the final likelihood for a large range of possible $K$ values, the minimal resulting BIC will be chosen as the optimal number of classes, $\hat{K}$ for our dataset. This approach is not ideal (see Discussion) but yields good results when applied to simulated data (see Results).

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
